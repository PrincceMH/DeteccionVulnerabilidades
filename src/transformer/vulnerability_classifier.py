"""
Módulo 3: Transformer - Vulnerability Classifier
=================================================

Este es el CORAZÓN del sistema de detección de vulnerabilidades.

Implementa un clasificador basado en CodeBERT (Transformer) que:

1. **Recibe** flujos tokenizados (source → sink)
2. **Procesa** a través de capas de atención
3. **Clasifica** si el flujo es una vulnerabilidad

Arquitectura del Modelo:
========================

    Input: [CLS] source_tokens [SEP] [FLOW] sink_tokens [SEP]
              │
              ▼
    ┌─────────────────────────────────────────┐
    │         Embedding Layer                  │
    │  (Token + Position + Type Embeddings)    │
    └─────────────────────────────────────────┘
              │
              ▼
    ┌─────────────────────────────────────────┐
    │      Transformer Encoder Layers          │
    │  (Multi-Head Self-Attention + FFN)       │
    │         × N capas (6 por defecto)        │
    └─────────────────────────────────────────┘
              │
              ▼
    ┌─────────────────────────────────────────┐
    │         [CLS] Representation             │
    │    (Vector de 768 dimensiones)           │
    └─────────────────────────────────────────┘
              │
              ▼
    ┌─────────────────────────────────────────┐
    │        Classification Head               │
    │  (Linear → Dropout → Linear → Softmax)   │
    └─────────────────────────────────────────┘
              │
              ▼
    Output: [P(benigno), P(malicioso)]


El modelo puede:
- Cargarse pre-entrenado de CodeBERT
- Entrenarse desde cero con datos propios
- Fine-tunearse con datos de DroidBench

"""

import math
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple, Any, Union
from enum import Enum, auto
from abc import ABC, abstractmethod
import json
from pathlib import Path

# Para compatibilidad sin PyTorch (diseño del modelo)
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import Dataset, DataLoader
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    # Clases dummy para documentación
    class nn:
        class Module:
            pass

from .code_tokenizer import (
    AndroidCodeTokenizer,
    TokenizedSequence,
    VocabularyBuilder,
    create_default_tokenizer
)
from .feature_extractor import (
    FeatureExtractor,
    ExtractedFeatures,
    SemanticFeatures,
    StructuralFeatures,
    SecurityFeatures
)


# ============================================================================
# CONFIGURACIÓN DEL MODELO
# ============================================================================

@dataclass
class TransformerConfig:
    """
    Configuración del modelo Transformer.
    
    Esta clase define todos los hiperparámetros del modelo.
    Los valores por defecto están optimizados para detección
    de vulnerabilidades Android.
    
    Attributes:
        vocab_size: Tamaño del vocabulario
        hidden_size: Dimensión de los embeddings (768 para CodeBERT)
        num_hidden_layers: Número de capas Transformer
        num_attention_heads: Cabezas de atención por capa
        intermediate_size: Tamaño de la capa FFN
        hidden_dropout_prob: Dropout en capas ocultas
        attention_dropout_prob: Dropout en atención
        max_position_embeddings: Máximo de posiciones
        num_labels: Número de clases (2: benigno/malicioso)
        
    Example:
        # Configuración por defecto (similar a CodeBERT-base)
        config = TransformerConfig()
        
        # Modelo más pequeño para pruebas
        config_small = TransformerConfig(
            hidden_size=256,
            num_hidden_layers=4,
            num_attention_heads=4
        )
    """
    # Tamaño del vocabulario (se ajusta dinámicamente)
    vocab_size: int = 50000
    
    # Dimensiones del modelo
    hidden_size: int = 768            # Tamaño de embeddings
    num_hidden_layers: int = 6        # Capas Transformer
    num_attention_heads: int = 12     # Cabezas de atención
    intermediate_size: int = 3072     # Capa FFN (4x hidden)
    
    # Regularización
    hidden_dropout_prob: float = 0.1
    attention_dropout_prob: float = 0.1
    
    # Embeddings
    max_position_embeddings: int = 512
    type_vocab_size: int = 2          # Source vs Sink
    
    # Clasificación
    num_labels: int = 2               # Benigno vs Malicioso
    
    # Características numéricas adicionales
    num_numeric_features: int = 12    # Del FeatureExtractor
    
    # Inicialización
    initializer_range: float = 0.02
    layer_norm_eps: float = 1e-12
    
    def to_dict(self) -> Dict[str, Any]:
        """Convierte configuración a diccionario."""
        return {
            "vocab_size": self.vocab_size,
            "hidden_size": self.hidden_size,
            "num_hidden_layers": self.num_hidden_layers,
            "num_attention_heads": self.num_attention_heads,
            "intermediate_size": self.intermediate_size,
            "hidden_dropout_prob": self.hidden_dropout_prob,
            "attention_dropout_prob": self.attention_dropout_prob,
            "max_position_embeddings": self.max_position_embeddings,
            "type_vocab_size": self.type_vocab_size,
            "num_labels": self.num_labels,
            "num_numeric_features": self.num_numeric_features
        }
    
    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'TransformerConfig':
        """Crea configuración desde diccionario."""
        return cls(**{k: v for k, v in config_dict.items() if k in cls.__dataclass_fields__})
    
    def save(self, path: Union[str, Path]):
        """Guarda configuración a archivo JSON."""
        path = Path(path)
        with open(path, 'w') as f:
            json.dump(self.to_dict(), f, indent=2)
    
    @classmethod
    def load(cls, path: Union[str, Path]) -> 'TransformerConfig':
        """Carga configuración desde archivo JSON."""
        path = Path(path)
        with open(path, 'r') as f:
            return cls.from_dict(json.load(f))


# Configuraciones predefinidas
class ModelSize(Enum):
    """Tamaños predefinidos del modelo."""
    TINY = auto()     # Para pruebas rápidas
    SMALL = auto()    # Modelo ligero
    BASE = auto()     # Tamaño estándar (CodeBERT)
    LARGE = auto()    # Modelo grande


def get_config_for_size(size: ModelSize) -> TransformerConfig:
    """
    Obtiene configuración predefinida según tamaño.
    
    Args:
        size: Tamaño del modelo
        
    Returns:
        TransformerConfig apropiada
    """
    configs = {
        ModelSize.TINY: TransformerConfig(
            hidden_size=128,
            num_hidden_layers=2,
            num_attention_heads=2,
            intermediate_size=512
        ),
        ModelSize.SMALL: TransformerConfig(
            hidden_size=256,
            num_hidden_layers=4,
            num_attention_heads=4,
            intermediate_size=1024
        ),
        ModelSize.BASE: TransformerConfig(
            hidden_size=768,
            num_hidden_layers=6,
            num_attention_heads=12,
            intermediate_size=3072
        ),
        ModelSize.LARGE: TransformerConfig(
            hidden_size=1024,
            num_hidden_layers=12,
            num_attention_heads=16,
            intermediate_size=4096
        )
    }
    return configs[size]


# ============================================================================
# COMPONENTES DEL TRANSFORMER
# ============================================================================

if TORCH_AVAILABLE:
    
    class Embeddings(nn.Module):
        """
        Capa de Embeddings del Transformer.
        
        Combina tres tipos de embeddings:
        1. Token Embeddings: Representación del token
        2. Position Embeddings: Información de posición
        3. Token Type Embeddings: Distinguir source vs sink
        
        Output = TokenEmb + PositionEmb + TypeEmb
        """
        
        def __init__(self, config: TransformerConfig):
            super().__init__()
            self.config = config
            
            # Embedding de tokens
            self.token_embeddings = nn.Embedding(
                config.vocab_size,
                config.hidden_size
            )
            
            # Embedding de posición
            self.position_embeddings = nn.Embedding(
                config.max_position_embeddings,
                config.hidden_size
            )
            
            # Embedding de tipo (source=0, sink=1)
            self.token_type_embeddings = nn.Embedding(
                config.type_vocab_size,
                config.hidden_size
            )
            
            # Normalización y dropout
            self.layer_norm = nn.LayerNorm(
                config.hidden_size,
                eps=config.layer_norm_eps
            )
            self.dropout = nn.Dropout(config.hidden_dropout_prob)
        
        def forward(
            self,
            input_ids: torch.Tensor,
            token_type_ids: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.Tensor] = None
        ) -> torch.Tensor:
            """
            Forward pass de embeddings.
            
            Args:
                input_ids: IDs de tokens [batch, seq_len]
                token_type_ids: IDs de tipo [batch, seq_len]
                position_ids: IDs de posición [batch, seq_len]
                
            Returns:
                Embeddings combinados [batch, seq_len, hidden]
            """
            seq_length = input_ids.size(1)
            
            # Crear position_ids si no se proporcionan
            if position_ids is None:
                position_ids = torch.arange(
                    seq_length,
                    dtype=torch.long,
                    device=input_ids.device
                )
                position_ids = position_ids.unsqueeze(0).expand_as(input_ids)
            
            # Crear token_type_ids si no se proporcionan
            if token_type_ids is None:
                token_type_ids = torch.zeros_like(input_ids)
            
            # Obtener embeddings
            token_embeds = self.token_embeddings(input_ids)
            position_embeds = self.position_embeddings(position_ids)
            type_embeds = self.token_type_embeddings(token_type_ids)
            
            # Combinar
            embeddings = token_embeds + position_embeds + type_embeds
            embeddings = self.layer_norm(embeddings)
            embeddings = self.dropout(embeddings)
            
            return embeddings
    
    
    class MultiHeadAttention(nn.Module):
        """
        Multi-Head Self-Attention.
        
        Esta es la operación clave del Transformer.
        Permite al modelo "atender" a diferentes partes
        de la secuencia simultáneamente.
        
        Attention(Q, K, V) = softmax(QK^T / √d_k) V
        
        Con múltiples cabezas, el modelo puede aprender
        diferentes tipos de relaciones.
        """
        
        def __init__(self, config: TransformerConfig):
            super().__init__()
            self.config = config
            
            # Verificar que hidden_size sea divisible
            assert config.hidden_size % config.num_attention_heads == 0
            
            self.num_heads = config.num_attention_heads
            self.head_size = config.hidden_size // config.num_attention_heads
            self.all_head_size = self.num_heads * self.head_size
            
            # Proyecciones lineales
            self.query = nn.Linear(config.hidden_size, self.all_head_size)
            self.key = nn.Linear(config.hidden_size, self.all_head_size)
            self.value = nn.Linear(config.hidden_size, self.all_head_size)
            
            # Output
            self.output = nn.Linear(config.hidden_size, config.hidden_size)
            
            # Dropout
            self.dropout = nn.Dropout(config.attention_dropout_prob)
        
        def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:
            """Reorganiza tensor para multi-head attention."""
            # [batch, seq, all_head] -> [batch, num_heads, seq, head_size]
            new_shape = x.size()[:-1] + (self.num_heads, self.head_size)
            x = x.view(*new_shape)
            return x.permute(0, 2, 1, 3)
        
        def forward(
            self,
            hidden_states: torch.Tensor,
            attention_mask: Optional[torch.Tensor] = None
        ) -> Tuple[torch.Tensor, torch.Tensor]:
            """
            Forward pass de atención.
            
            Args:
                hidden_states: Estados ocultos [batch, seq, hidden]
                attention_mask: Máscara de atención [batch, 1, 1, seq]
                
            Returns:
                Tuple de (output, attention_weights)
            """
            # Proyecciones Q, K, V
            query_layer = self.transpose_for_scores(self.query(hidden_states))
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))
            
            # Attention scores: QK^T / sqrt(d_k)
            attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
            attention_scores = attention_scores / math.sqrt(self.head_size)
            
            # Aplicar máscara
            if attention_mask is not None:
                attention_scores = attention_scores + attention_mask
            
            # Softmax para probabilidades
            attention_probs = F.softmax(attention_scores, dim=-1)
            attention_probs = self.dropout(attention_probs)
            
            # Aplicar a values
            context_layer = torch.matmul(attention_probs, value_layer)
            
            # Reorganizar de vuelta
            # [batch, heads, seq, head_size] -> [batch, seq, hidden]
            context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
            new_shape = context_layer.size()[:-2] + (self.all_head_size,)
            context_layer = context_layer.view(*new_shape)
            
            # Proyección final
            output = self.output(context_layer)
            
            return output, attention_probs
    
    
    class FeedForward(nn.Module):
        """
        Feed-Forward Network (FFN) del Transformer.
        
        FFN(x) = GELU(xW1 + b1)W2 + b2
        
        Esta capa procesa cada posición independientemente,
        expandiendo y luego comprimiendo la representación.
        """
        
        def __init__(self, config: TransformerConfig):
            super().__init__()
            
            self.dense1 = nn.Linear(config.hidden_size, config.intermediate_size)
            self.dense2 = nn.Linear(config.intermediate_size, config.hidden_size)
            self.dropout = nn.Dropout(config.hidden_dropout_prob)
        
        def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
            """Forward pass del FFN."""
            hidden_states = self.dense1(hidden_states)
            hidden_states = F.gelu(hidden_states)
            hidden_states = self.dense2(hidden_states)
            hidden_states = self.dropout(hidden_states)
            return hidden_states
    
    
    class TransformerLayer(nn.Module):
        """
        Una capa completa del Transformer.
        
        Estructura:
            x → Attention → Add & Norm → FFN → Add & Norm → output
        
        Usa conexiones residuales y layer normalization
        para estabilidad del entrenamiento.
        """
        
        def __init__(self, config: TransformerConfig):
            super().__init__()
            
            self.attention = MultiHeadAttention(config)
            self.feed_forward = FeedForward(config)
            
            self.attention_norm = nn.LayerNorm(
                config.hidden_size,
                eps=config.layer_norm_eps
            )
            self.output_norm = nn.LayerNorm(
                config.hidden_size,
                eps=config.layer_norm_eps
            )
            
            self.dropout = nn.Dropout(config.hidden_dropout_prob)
        
        def forward(
            self,
            hidden_states: torch.Tensor,
            attention_mask: Optional[torch.Tensor] = None
        ) -> Tuple[torch.Tensor, torch.Tensor]:
            """
            Forward pass de una capa.
            
            Args:
                hidden_states: Estados [batch, seq, hidden]
                attention_mask: Máscara [batch, 1, 1, seq]
                
            Returns:
                Tuple de (output, attention_weights)
            """
            # Self-Attention con residual
            attention_output, attention_weights = self.attention(
                hidden_states, attention_mask
            )
            hidden_states = self.attention_norm(hidden_states + attention_output)
            
            # Feed-Forward con residual
            ff_output = self.feed_forward(hidden_states)
            hidden_states = self.output_norm(hidden_states + ff_output)
            
            return hidden_states, attention_weights
    
    
    class TransformerEncoder(nn.Module):
        """
        Encoder completo del Transformer.
        
        Apila múltiples capas TransformerLayer.
        """
        
        def __init__(self, config: TransformerConfig):
            super().__init__()
            
            self.layers = nn.ModuleList([
                TransformerLayer(config) 
                for _ in range(config.num_hidden_layers)
            ])
        
        def forward(
            self,
            hidden_states: torch.Tensor,
            attention_mask: Optional[torch.Tensor] = None,
            output_attentions: bool = False
        ) -> Tuple[torch.Tensor, Optional[List[torch.Tensor]]]:
            """
            Forward pass del encoder.
            
            Args:
                hidden_states: Embeddings [batch, seq, hidden]
                attention_mask: Máscara [batch, 1, 1, seq]
                output_attentions: Si retornar pesos de atención
                
            Returns:
                Tuple de (output, all_attentions)
            """
            all_attentions = [] if output_attentions else None
            
            for layer in self.layers:
                hidden_states, attention_weights = layer(
                    hidden_states, attention_mask
                )
                if output_attentions:
                    all_attentions.append(attention_weights)
            
            return hidden_states, all_attentions
    
    
    class ClassificationHead(nn.Module):
        """
        Cabeza de clasificación.
        
        Toma la representación [CLS] y produce probabilidades
        de clasificación.
        
        [CLS] embedding → Dense → Dropout → Output → Softmax
        """
        
        def __init__(self, config: TransformerConfig):
            super().__init__()
            
            self.num_numeric_features = config.num_numeric_features
            
            # Capa intermedia - acepta hidden_size o hidden_size + numeric_features
            self.dense = nn.Linear(config.hidden_size, config.hidden_size)
            self.dense_with_features = nn.Linear(
                config.hidden_size + config.num_numeric_features,
                config.hidden_size
            )
            self.dropout = nn.Dropout(config.hidden_dropout_prob)
            
            # Capa de salida
            self.output = nn.Linear(config.hidden_size, config.num_labels)
        
        def forward(
            self,
            cls_embedding: torch.Tensor,
            numeric_features: Optional[torch.Tensor] = None
        ) -> torch.Tensor:
            """
            Forward pass de clasificación.
            
            Args:
                cls_embedding: Embedding [CLS] [batch, hidden]
                numeric_features: Features numéricas [batch, num_features]
                
            Returns:
                Logits de clasificación [batch, num_labels]
            """
            # Usar capa apropiada segun si hay features numericas
            if numeric_features is not None:
                cls_embedding = torch.cat([cls_embedding, numeric_features], dim=-1)
                hidden = self.dense_with_features(cls_embedding)
            else:
                hidden = self.dense(cls_embedding)
            
            hidden = F.gelu(hidden)
            hidden = self.dropout(hidden)
            logits = self.output(hidden)
            
            return logits


# ============================================================================
# MODELO PRINCIPAL
# ============================================================================

if TORCH_AVAILABLE:
    
    class VulnerabilityClassifier(nn.Module):
        """
        Clasificador de Vulnerabilidades basado en Transformer.
        
        Este es el MODELO PRINCIPAL del sistema.
        
        Toma flujos tokenizados (source → sink) y predice si
        representan una vulnerabilidad de seguridad.
        
        Arquitectura:
            Embeddings → Transformer Encoder → [CLS] → Classification Head
        
        Uso básico:
            # Crear modelo
            config = TransformerConfig()
            model = VulnerabilityClassifier(config)
            
            # Tokenizar flujo
            tokenizer = AndroidCodeTokenizer()
            sequence = tokenizer.tokenize_flow(
                source="TelephonyManager.getDeviceId",
                sink="SmsManager.sendTextMessage"
            )
            
            # Predecir
            inputs = {
                "input_ids": torch.tensor([sequence.input_ids]),
                "attention_mask": torch.tensor([sequence.attention_mask]),
                "token_type_ids": torch.tensor([sequence.token_type_ids])
            }
            outputs = model(**inputs)
            prediction = outputs["prediction"]  # 0=benigno, 1=malicioso
        
        Entrenamiento:
            # Con datos etiquetados
            outputs = model(
                input_ids=batch_input_ids,
                attention_mask=batch_attention_mask,
                labels=batch_labels  # 0 o 1
            )
            loss = outputs["loss"]
            loss.backward()
        """
        
        def __init__(self, config: TransformerConfig):
            """
            Inicializa el modelo.
            
            Args:
                config: Configuración del Transformer
            """
            super().__init__()
            
            self.config = config
            
            # Componentes
            self.embeddings = Embeddings(config)
            self.encoder = TransformerEncoder(config)
            self.classifier = ClassificationHead(config)
            
            # Inicializar pesos
            self.apply(self._init_weights)
        
        def _init_weights(self, module):
            """Inicializa pesos del modelo."""
            if isinstance(module, nn.Linear):
                module.weight.data.normal_(
                    mean=0.0, std=self.config.initializer_range
                )
                if module.bias is not None:
                    module.bias.data.zero_()
            elif isinstance(module, nn.Embedding):
                module.weight.data.normal_(
                    mean=0.0, std=self.config.initializer_range
                )
            elif isinstance(module, nn.LayerNorm):
                module.bias.data.zero_()
                module.weight.data.fill_(1.0)
        
        def _create_attention_mask(
            self,
            attention_mask: torch.Tensor
        ) -> torch.Tensor:
            """
            Crea máscara de atención extendida.
            
            Convierte máscara [batch, seq] a [batch, 1, 1, seq]
            con -inf en posiciones de padding.
            """
            # Expandir dimensiones
            extended_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            
            # Convertir 0s a -inf para softmax
            extended_mask = (1.0 - extended_mask) * -10000.0
            
            return extended_mask
        
        def forward(
            self,
            input_ids: torch.Tensor,
            attention_mask: Optional[torch.Tensor] = None,
            token_type_ids: Optional[torch.Tensor] = None,
            numeric_features: Optional[torch.Tensor] = None,
            labels: Optional[torch.Tensor] = None,
            output_attentions: bool = False
        ) -> Dict[str, Any]:
            """
            Forward pass del modelo.
            
            Args:
                input_ids: IDs de tokens [batch, seq_len]
                attention_mask: Máscara de padding [batch, seq_len]
                token_type_ids: IDs de tipo [batch, seq_len]
                numeric_features: Features numéricas [batch, num_features]
                labels: Etiquetas para entrenamiento [batch]
                output_attentions: Si retornar pesos de atención
                
            Returns:
                Dict con:
                - logits: Logits de clasificación [batch, num_labels]
                - probabilities: Probabilidades [batch, num_labels]
                - prediction: Predicción [batch]
                - loss: Pérdida (si se dan labels)
                - attentions: Pesos de atención (si se solicitan)
            """
            batch_size = input_ids.size(0)
            
            # Crear máscara por defecto si no se proporciona
            if attention_mask is None:
                attention_mask = torch.ones_like(input_ids)
            
            # Extender máscara de atención
            extended_attention_mask = self._create_attention_mask(attention_mask)
            
            # Embeddings
            embeddings = self.embeddings(
                input_ids=input_ids,
                token_type_ids=token_type_ids
            )
            
            # Transformer Encoder
            encoder_output, attentions = self.encoder(
                hidden_states=embeddings,
                attention_mask=extended_attention_mask,
                output_attentions=output_attentions
            )
            
            # Obtener embedding [CLS] (primera posición)
            cls_embedding = encoder_output[:, 0, :]
            
            # Clasificación
            logits = self.classifier(
                cls_embedding=cls_embedding,
                numeric_features=numeric_features
            )
            
            # Probabilidades y predicción
            probabilities = F.softmax(logits, dim=-1)
            prediction = torch.argmax(probabilities, dim=-1)
            
            # Preparar output
            output = {
                "logits": logits,
                "probabilities": probabilities,
                "prediction": prediction,
                "cls_embedding": cls_embedding
            }
            
            # Calcular pérdida si hay labels
            if labels is not None:
                loss_fn = nn.CrossEntropyLoss()
                loss = loss_fn(logits, labels)
                output["loss"] = loss
            
            # Añadir atenciones si se solicitan
            if output_attentions:
                output["attentions"] = attentions
            
            return output
        
        def predict(
            self,
            input_ids: torch.Tensor,
            attention_mask: Optional[torch.Tensor] = None,
            token_type_ids: Optional[torch.Tensor] = None,
            numeric_features: Optional[torch.Tensor] = None
        ) -> Tuple[torch.Tensor, torch.Tensor]:
            """
            Hace predicción sin gradientes.
            
            Args:
                input_ids: IDs de tokens [batch, seq_len]
                attention_mask: Máscara [batch, seq_len]
                token_type_ids: Tipos [batch, seq_len]
                numeric_features: Features numéricas [batch, num_features]
                
            Returns:
                Tuple de (predictions, probabilities)
            """
            self.eval()
            with torch.no_grad():
                outputs = self.forward(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    token_type_ids=token_type_ids,
                    numeric_features=numeric_features
                )
            return outputs["prediction"], outputs["probabilities"]
        
        def save_model(self, path: Union[str, Path]):
            """
            Guarda el modelo a archivo.
            
            Args:
                path: Ruta donde guardar
            """
            path = Path(path)
            path.mkdir(parents=True, exist_ok=True)
            
            # Guardar pesos
            torch.save(self.state_dict(), path / "model.pt")
            
            # Guardar configuración
            self.config.save(path / "config.json")
        
        @classmethod
        def load_model(cls, path: Union[str, Path]) -> 'VulnerabilityClassifier':
            """
            Carga un modelo guardado.
            
            Args:
                path: Ruta del modelo guardado
                
            Returns:
                Modelo cargado
            """
            path = Path(path)
            
            # Cargar configuración
            config = TransformerConfig.load(path / "config.json")
            
            # Crear modelo
            model = cls(config)
            
            # Cargar pesos
            model.load_state_dict(torch.load(path / "model.pt"))
            
            return model


# ============================================================================
# DATASET Y DATALOADER
# ============================================================================

if TORCH_AVAILABLE:
    
    class VulnerabilityDataset(Dataset):
        """
        Dataset para entrenamiento del clasificador.
        
        Toma flujos extraídos y los prepara para el modelo.
        
        Ejemplo:
            # Crear dataset
            dataset = VulnerabilityDataset(
                flows=extracted_features,
                tokenizer=tokenizer,
                labels=[0, 1, 1, 0, ...]  # Etiquetas
            )
            
            # Crear DataLoader
            loader = DataLoader(dataset, batch_size=32, shuffle=True)
            
            # Iterar
            for batch in loader:
                outputs = model(**batch)
        """
        
        def __init__(
            self,
            flows: List[ExtractedFeatures],
            tokenizer: AndroidCodeTokenizer,
            labels: Optional[List[int]] = None
        ):
            """
            Inicializa el dataset.
            
            Args:
                flows: Lista de características extraídas
                tokenizer: Tokenizador a usar
                labels: Etiquetas (0=benigno, 1=malicioso)
            """
            self.flows = flows
            self.tokenizer = tokenizer
            self.labels = labels
            
            # Preparar datos
            self._prepare_data()
        
        def _prepare_data(self):
            """Prepara todos los datos para acceso rápido."""
            self.tokenized = []
            self.numeric_features = []
            
            for flow in self.flows:
                # Tokenizar
                seq = self.tokenizer.tokenize_flow(
                    source=flow.semantic.source_api,
                    sink=flow.semantic.sink_api,
                    category=flow.security.category,
                    risk_level="HIGH" if flow.security.risk_level >= 7 else "MEDIUM",
                    permissions=flow.security.permissions_required
                )
                self.tokenized.append(seq)
                
                # Features numéricas
                self.numeric_features.append(flow.get_combined_vector())
        
        def __len__(self) -> int:
            return len(self.flows)
        
        def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
            """
            Obtiene un item del dataset.
            
            Returns:
                Dict con tensores para el modelo
            """
            seq = self.tokenized[idx]
            
            item = {
                "input_ids": torch.tensor(seq.input_ids, dtype=torch.long),
                "attention_mask": torch.tensor(seq.attention_mask, dtype=torch.long),
                "token_type_ids": torch.tensor(seq.token_type_ids, dtype=torch.long),
                "numeric_features": torch.tensor(
                    self.numeric_features[idx], dtype=torch.float
                )
            }
            
            if self.labels is not None:
                item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
            
            return item


# ============================================================================
# CLASIFICADOR DE ALTO NIVEL (API SIMPLIFICADA)
# ============================================================================

class VulnerabilityPrediction:
    """
    Resultado de una predicción de vulnerabilidad.
    
    Attributes:
        flow_id: ID del flujo analizado
        is_vulnerable: Si el flujo es una vulnerabilidad
        confidence: Confianza de la predicción (0-1)
        probability_benign: P(benigno)
        probability_malicious: P(malicioso)
        risk_assessment: Evaluación de riesgo
    """
    
    def __init__(
        self,
        flow_id: str,
        is_vulnerable: bool,
        confidence: float,
        probability_benign: float,
        probability_malicious: float,
        category: str = "",
        risk_level: int = 0
    ):
        self.flow_id = flow_id
        self.is_vulnerable = is_vulnerable
        self.confidence = confidence
        self.probability_benign = probability_benign
        self.probability_malicious = probability_malicious
        self.category = category
        self.risk_level = risk_level
    
    @property
    def risk_assessment(self) -> str:
        """Evaluación del riesgo."""
        if not self.is_vulnerable:
            return "BAJO - Probablemente benigno"
        elif self.confidence > 0.9:
            return "CRÍTICO - Alta confianza de vulnerabilidad"
        elif self.confidence > 0.7:
            return "ALTO - Probable vulnerabilidad"
        else:
            return "MEDIO - Posible vulnerabilidad"
    
    def __str__(self) -> str:
        status = "⚠️ VULNERABILIDAD" if self.is_vulnerable else "✓ Benigno"
        return (
            f"{status} [{self.flow_id}]\n"
            f"  Confianza: {self.confidence:.1%}\n"
            f"  Categoría: {self.category}\n"
            f"  Evaluación: {self.risk_assessment}"
        )


class AndroidVulnerabilityDetector:
    """
    API de alto nivel para detección de vulnerabilidades.
    
    Esta clase proporciona una interfaz simple para usar
    todo el pipeline de detección.
    
    Ejemplo completo:
        # Crear detector
        detector = AndroidVulnerabilityDetector()
        
        # Analizar flujo
        result = detector.analyze_flow(
            source="TelephonyManager.getDeviceId",
            sink="SmsManager.sendTextMessage"
        )
        
        print(result)
        # ⚠️ VULNERABILIDAD [flow_001]
        #   Confianza: 95.2%
        #   Categoría: SMS_LEAK
        #   Evaluación: CRÍTICO - Alta confianza de vulnerabilidad
        
        # Analizar múltiples flujos
        results = detector.analyze_flows(list_of_flows)
    """
    
    def __init__(
        self,
        model_path: Optional[Union[str, Path]] = None,
        config: Optional[TransformerConfig] = None,
        device: str = "auto"
    ):
        """
        Inicializa el detector.
        
        Args:
            model_path: Ruta a modelo pre-entrenado (None = modelo nuevo)
            config: Configuración del modelo
            device: Dispositivo ('cpu', 'cuda', 'auto')
        """
        self.config = config or TransformerConfig()
        self.tokenizer = create_default_tokenizer()
        self.feature_extractor = FeatureExtractor()
        
        # Determinar dispositivo
        if device == "auto":
            self.device = "cuda" if TORCH_AVAILABLE and torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        # Cargar o crear modelo
        if TORCH_AVAILABLE:
            if model_path:
                self.model = VulnerabilityClassifier.load_model(model_path)
            else:
                self.model = VulnerabilityClassifier(self.config)
            
            self.model.to(self.device)
            self.model.eval()
        else:
            self.model = None
    
    def analyze_flow(
        self,
        source: str,
        sink: str,
        category: Optional[str] = None,
        risk_level: Optional[int] = None,
        permissions: Optional[List[str]] = None
    ) -> VulnerabilityPrediction:
        """
        Analiza un flujo individual.
        
        Args:
            source: API source
            sink: API sink
            category: Categoría de vulnerabilidad
            risk_level: Nivel de riesgo previo
            permissions: Permisos involucrados
            
        Returns:
            VulnerabilityPrediction con el resultado
        """
        # Tokenizar
        sequence = self.tokenizer.tokenize_flow(
            source=source,
            sink=sink,
            category=category,
            risk_level="HIGH" if risk_level and risk_level >= 7 else "MEDIUM",
            permissions=permissions
        )
        
        if not TORCH_AVAILABLE or self.model is None:
            # Sin PyTorch, usar heurística
            return self._heuristic_prediction(
                source, sink, category, risk_level
            )
        
        # Preparar input
        input_ids = torch.tensor([sequence.input_ids], device=self.device)
        attention_mask = torch.tensor([sequence.attention_mask], device=self.device)
        token_type_ids = torch.tensor([sequence.token_type_ids], device=self.device)
        
        # Predecir
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids
            )
        
        probs = outputs["probabilities"][0].cpu().numpy()
        pred = outputs["prediction"][0].item()
        
        return VulnerabilityPrediction(
            flow_id=f"flow_{hash(source + sink) % 10000:04d}",
            is_vulnerable=pred == 1,
            confidence=float(probs[pred]),
            probability_benign=float(probs[0]),
            probability_malicious=float(probs[1]),
            category=category or "UNKNOWN",
            risk_level=risk_level or 5
        )
    
    def _heuristic_prediction(
        self,
        source: str,
        sink: str,
        category: Optional[str],
        risk_level: Optional[int]
    ) -> VulnerabilityPrediction:
        """
        Predicción heurística cuando no hay modelo.
        
        Usa reglas basadas en conocimiento de seguridad Android.
        """
        # Patrones peligrosos conocidos
        dangerous_patterns = [
            ("getDeviceId", "sendTextMessage"),
            ("getDeviceId", "openConnection"),
            ("getLocation", "sendTextMessage"),
            ("getLocation", "openConnection"),
            ("query", "openConnection"),
            ("getLine1Number", "sendTextMessage"),
        ]
        
        # Verificar patrones
        source_method = source.split('.')[-1] if '.' in source else source
        sink_method = sink.split('.')[-1] if '.' in sink else sink
        
        is_dangerous = any(
            s in source_method and k in sink_method
            for s, k in dangerous_patterns
        )
        
        # Calcular confianza basada en heurísticas
        confidence = 0.5
        if is_dangerous:
            confidence = 0.85
        if risk_level and risk_level >= 7:
            confidence = min(confidence + 0.1, 0.95)
        if category and "LEAK" in str(category):
            confidence = min(confidence + 0.05, 0.95)
        
        return VulnerabilityPrediction(
            flow_id=f"flow_{hash(source + sink) % 10000:04d}",
            is_vulnerable=is_dangerous or (risk_level and risk_level >= 7),
            confidence=confidence,
            probability_benign=1 - confidence if is_dangerous else confidence,
            probability_malicious=confidence if is_dangerous else 1 - confidence,
            category=category or "UNKNOWN",
            risk_level=risk_level or 5
        )
    
    def analyze_flows(
        self,
        flows: List[Dict[str, Any]]
    ) -> List[VulnerabilityPrediction]:
        """
        Analiza múltiples flujos.
        
        Args:
            flows: Lista de dicts con 'source', 'sink', etc.
            
        Returns:
            Lista de VulnerabilityPrediction
        """
        return [
            self.analyze_flow(
                source=f.get("source", ""),
                sink=f.get("sink", ""),
                category=f.get("category"),
                risk_level=f.get("risk_level"),
                permissions=f.get("permissions")
            )
            for f in flows
        ]
    
    def get_summary(
        self,
        predictions: List[VulnerabilityPrediction]
    ) -> Dict[str, Any]:
        """
        Obtiene resumen de predicciones.
        
        Args:
            predictions: Lista de predicciones
            
        Returns:
            Resumen con estadísticas
        """
        if not predictions:
            return {"total": 0}
        
        vulnerabilities = [p for p in predictions if p.is_vulnerable]
        
        return {
            "total_flows": len(predictions),
            "vulnerabilities_found": len(vulnerabilities),
            "benign_flows": len(predictions) - len(vulnerabilities),
            "vulnerability_rate": len(vulnerabilities) / len(predictions),
            "high_confidence_vulnerabilities": len([
                p for p in vulnerabilities if p.confidence > 0.8
            ]),
            "categories": list(set(p.category for p in vulnerabilities))
        }


# ============================================================================
# FUNCIONES DE UTILIDAD
# ============================================================================

def create_classifier(
    size: ModelSize = ModelSize.BASE,
    vocab_size: Optional[int] = None
) -> 'VulnerabilityClassifier':
    """
    Crea un clasificador con configuración predefinida.
    
    Args:
        size: Tamaño del modelo
        vocab_size: Tamaño del vocabulario (opcional)
        
    Returns:
        VulnerabilityClassifier configurado
    """
    config = get_config_for_size(size)
    if vocab_size:
        config.vocab_size = vocab_size
    
    if TORCH_AVAILABLE:
        return VulnerabilityClassifier(config)
    else:
        raise ImportError(
            "PyTorch no está instalado. "
            "Instala con: pip install torch"
        )


def create_detector(
    model_path: Optional[str] = None
) -> AndroidVulnerabilityDetector:
    """
    Crea un detector de vulnerabilidades.
    
    Args:
        model_path: Ruta a modelo pre-entrenado
        
    Returns:
        AndroidVulnerabilityDetector listo para usar
    """
    return AndroidVulnerabilityDetector(model_path=model_path)
