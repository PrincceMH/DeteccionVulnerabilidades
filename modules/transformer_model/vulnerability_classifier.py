"""
MÓDULO 3: Modelo Transformer para Clasificación de Vulnerabilidades
Basado en CodeBERT fine-tuneado específicamente para Android
"""

import torch
import torch.nn as nn
from transformers import RobertaTokenizer, RobertaModel, RobertaConfig
from typing import Dict, List, Any, Tuple
import logging
import numpy as np
from pathlib import Path


class VulnerabilityClassifier(nn.Module):
    """
    Clasificador de vulnerabilidades basado en CodeBERT
    - Fine-tuning específico para patrones Android
    - Embeddings especializados para APIs, permisos y componentes
    - Scoring contextual de criticidad [0, 1]
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        
        self.config = config
        self.logger = logging.getLogger("VulnerabilityClassifier")
        
        # Configuración del modelo
        self.model_name = config.get('model_name', 'microsoft/codebert-base')
        self.model_path = config.get('model_path', 'models/codebert_finetuned/')
        self.max_length = config.get('max_sequence_length', 512)
        self.hidden_size = config.get('hidden_size', 512)
        
        # Umbrales de clasificación
        self.threshold_critical = config.get('threshold_critical', 0.75)
        self.threshold_high = config.get('threshold_high', 0.55)
        self.threshold_medium = config.get('threshold_medium', 0.35)
        self.threshold_low = config.get('threshold_low', 0.20)
        
        # Inicializar componentes del modelo
        self._initialize_model()
        
        self.logger.info(f"Modelo inicializado: {self.model_name}")
    
    def _initialize_model(self):
        """Inicializa arquitectura del modelo"""
        
        # 1. Tokenizer CodeBERT
        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)
        
        # Agregar tokens especiales para Android
        special_tokens = self._get_android_special_tokens()
        self.tokenizer.add_tokens(special_tokens)
        
        # 2. Modelo base CodeBERT
        try:
            # Intentar cargar modelo fine-tuneado
            model_path = Path(self.model_path)
            if model_path.exists():
                self.logger.info(f"Cargando modelo fine-tuneado desde {self.model_path}")
                self.encoder = RobertaModel.from_pretrained(self.model_path)
            else:
                self.logger.info("Cargando modelo base CodeBERT")
                self.encoder = RobertaModel.from_pretrained(self.model_name)
                
        except Exception as e:
            self.logger.warning(f"Error cargando modelo: {e}. Usando configuración base.")
            config = RobertaConfig.from_pretrained(self.model_name)
            self.encoder = RobertaModel(config)
        
        # Ajustar embeddings para nuevos tokens
        self.encoder.resize_token_embeddings(len(self.tokenizer))
        
        # 3. Embeddings especializados adicionales
        self.permission_embedding = nn.Embedding(
            num_embeddings=100,  # Máximo 100 permisos diferentes
            embedding_dim=self.config.get('permission_embedding_dim', 64)
        )
        
        self.component_embedding = nn.Embedding(
            num_embeddings=10,  # Tipos de componentes Android
            embedding_dim=self.config.get('component_embedding_dim', 32)
        )
        
        # 4. Capas de clasificación
        total_dim = self.hidden_size + 64 + 32  # CodeBERT + permission + component
        
        self.classifier = nn.Sequential(
            nn.Linear(total_dim, 256),
            nn.ReLU(),
            nn.Dropout(self.config.get('dropout', 0.3)),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(self.config.get('dropout', 0.3)),
            nn.Linear(128, 1),  # Score binario [0, 1]
            nn.Sigmoid()
        )
        
        # 5. Dispositivo (CPU/GPU)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.to(self.device)
        
        self.logger.info(f"Modelo configurado en dispositivo: {self.device}")
    
    def _get_android_special_tokens(self) -> List[str]:
        """Define tokens especiales para APIs Android críticas"""
        special_tokens = [
            # Componentes
            "[ACTIVITY]", "[SERVICE]", "[RECEIVER]", "[PROVIDER]",
            
            # Sources críticos
            "[SOURCE_LOCATION]", "[SOURCE_CONTACTS]", "[SOURCE_DEVICE_ID]",
            "[SOURCE_CAMERA]", "[SOURCE_MICROPHONE]",
            
            # Sinks peligrosos
            "[SINK_NETWORK]", "[SINK_WEBVIEW]", "[SINK_LOG]", "[SINK_INTENT]",
            "[SINK_STORAGE]",
            
            # Operaciones
            "[ENCRYPT]", "[DECRYPT]", "[REFLECTION]", "[NATIVE]",
            
            # Flujos
            "[FLOW_START]", "[FLOW_END]", "[INTER_COMPONENT]"
        ]
        
        return special_tokens
    
    def classify_flow(self, flow: Any) -> float:
        """
        Clasifica un flujo taint y retorna score de criticidad
        
        Args:
            flow: Objeto TaintFlow del módulo taint_analysis
            
        Returns:
            Score de criticidad [0, 1]
        """
        self.eval()
        
        with torch.no_grad():
            try:
                # 1. Preparar input
                input_data = self._prepare_input(flow)
                
                # 2. Forward pass
                score = self.forward(
                    input_ids=input_data['input_ids'],
                    attention_mask=input_data['attention_mask'],
                    permission_ids=input_data['permission_ids'],
                    component_ids=input_data['component_ids']
                )
                
                return score.item()
                
            except Exception as e:
                self.logger.error(f"Error en clasificación: {str(e)}")
                # Retornar score por defecto basado en heurística
                return flow.criticality_score
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        permission_ids: torch.Tensor,
        component_ids: torch.Tensor
    ) -> torch.Tensor:
        """
        Forward pass del modelo
        
        Args:
            input_ids: Token IDs de la secuencia de código [batch, seq_len]
            attention_mask: Máscara de atención [batch, seq_len]
            permission_ids: IDs de permisos [batch, num_perms]
            component_ids: IDs de componentes [batch, num_components]
            
        Returns:
            Scores de criticidad [batch, 1]
        """
        # 1. Encodear secuencia de código con CodeBERT
        encoder_outputs = self.encoder(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # Usar [CLS] token como representación de la secuencia
        code_representation = encoder_outputs.last_hidden_state[:, 0, :]  # [batch, hidden_size]
        
        # 2. Embeddings de permisos (promedio)
        perm_embeds = self.permission_embedding(permission_ids)  # [batch, num_perms, perm_dim]
        perm_representation = perm_embeds.mean(dim=1)  # [batch, perm_dim]
        
        # 3. Embeddings de componentes (promedio)
        comp_embeds = self.component_embedding(component_ids)  # [batch, num_comps, comp_dim]
        comp_representation = comp_embeds.mean(dim=1)  # [batch, comp_dim]
        
        # 4. Concatenar todas las representaciones
        combined = torch.cat([
            code_representation,
            perm_representation,
            comp_representation
        ], dim=-1)  # [batch, total_dim]
        
        # 5. Clasificación final
        scores = self.classifier(combined)  # [batch, 1]
        
        return scores.squeeze(-1)  # [batch]
    
    def _prepare_input(self, flow: Any) -> Dict[str, torch.Tensor]:
        """
        Prepara input para el modelo desde un TaintFlow
        
        Returns:
            Diccionario con tensores listos para el modelo
        """
        # 1. Construir secuencia textual del flujo
        flow_sequence = self._flow_to_sequence(flow)
        
        # 2. Tokenizar
        encoding = self.tokenizer(
            flow_sequence,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # 3. Preparar permisos
        permission_ids = self._encode_permissions(flow.permissions)
        
        # 4. Preparar componentes
        component_ids = self._encode_components(flow.components)
        
        return {
            'input_ids': encoding['input_ids'].to(self.device),
            'attention_mask': encoding['attention_mask'].to(self.device),
            'permission_ids': permission_ids.to(self.device),
            'component_ids': component_ids.to(self.device)
        }
    
    def _flow_to_sequence(self, flow: Any) -> str:
        """
        Convierte TaintFlow a secuencia textual para el modelo
        
        Format:
        [FLOW_START] <source> [SEP] <api_1> [SEP] ... [SEP] <sink> [FLOW_END]
        """
        sequence_parts = ["[FLOW_START]"]
        
        # Agregar source con token especial
        source_token = self._get_source_token(flow.source)
        sequence_parts.append(f"{source_token} {flow.source}")
        
        # Agregar secuencia de APIs
        for api in flow.api_sequence:
            # Simplificar API para tokenización
            api_simple = self._simplify_api(api)
            sequence_parts.append(api_simple)
        
        # Agregar sink con token especial
        sink_token = self._get_sink_token(flow.sink)
        sequence_parts.append(f"{sink_token} {flow.sink}")
        
        # Agregar metadata relevante
        if flow.metadata.get('has_reflection'):
            sequence_parts.append("[REFLECTION]")
        if flow.metadata.get('has_encryption'):
            sequence_parts.append("[ENCRYPT]")
        if flow.metadata.get('inter_component'):
            sequence_parts.append("[INTER_COMPONENT]")
        
        sequence_parts.append("[FLOW_END]")
        
        return " [SEP] ".join(sequence_parts)
    
    def _get_source_token(self, source: str) -> str:
        """Mapea source a token especial"""
        if 'Location' in source:
            return "[SOURCE_LOCATION]"
        elif 'Contact' in source:
            return "[SOURCE_CONTACTS]"
        elif 'DeviceId' in source or 'ANDROID_ID' in source:
            return "[SOURCE_DEVICE_ID]"
        elif 'Camera' in source:
            return "[SOURCE_CAMERA]"
        elif 'MediaRecorder' in source:
            return "[SOURCE_MICROPHONE]"
        else:
            return "[SOURCE]"
    
    def _get_sink_token(self, sink: str) -> str:
        """Mapea sink a token especial"""
        if 'Http' in sink or 'OkHttp' in sink:
            return "[SINK_NETWORK]"
        elif 'WebView' in sink:
            return "[SINK_WEBVIEW]"
        elif 'Log' in sink:
            return "[SINK_LOG]"
        elif 'Intent' in sink:
            return "[SINK_INTENT]"
        elif 'File' in sink:
            return "[SINK_STORAGE]"
        else:
            return "[SINK]"
    
    def _simplify_api(self, api: str) -> str:
        """Simplifica firma de API para tokenización"""
        # Extraer clase y método
        if ':' in api:
            parts = api.split(':')
            class_name = parts[0].split('.')[-1]  # Última parte del package
            method_name = parts[1].strip().split('(')[0].strip()  # Método sin parámetros
            return f"{class_name}.{method_name}"
        return api
    
    def _encode_permissions(self, permissions: List[str]) -> torch.Tensor:
        """
        Codifica permisos Android a IDs
        
        Returns:
            Tensor [1, num_perms]
        """
        # Mapeo simple de permisos a IDs
        perm_to_id = {
            'INTERNET': 1,
            'ACCESS_FINE_LOCATION': 2,
            'ACCESS_COARSE_LOCATION': 3,
            'READ_CONTACTS': 4,
            'WRITE_CONTACTS': 5,
            'CAMERA': 6,
            'RECORD_AUDIO': 7,
            'READ_PHONE_STATE': 8,
            'SEND_SMS': 9,
            'READ_EXTERNAL_STORAGE': 10,
            'WRITE_EXTERNAL_STORAGE': 11
        }
        
        perm_ids = []
        for perm in permissions:
            for key, value in perm_to_id.items():
                if key in perm:
                    perm_ids.append(value)
                    break
        
        # Padding
        max_perms = 10
        while len(perm_ids) < max_perms:
            perm_ids.append(0)  # 0 = padding
        
        perm_ids = perm_ids[:max_perms]
        
        return torch.tensor([perm_ids], dtype=torch.long)
    
    def _encode_components(self, components: List[str]) -> torch.Tensor:
        """
        Codifica componentes Android a IDs
        
        Returns:
            Tensor [1, num_components]
        """
        comp_to_id = {
            'Activity': 1,
            'Service': 2,
            'Receiver': 3,
            'Provider': 4
        }
        
        comp_ids = []
        for comp in components:
            for key, value in comp_to_id.items():
                if key in comp:
                    comp_ids.append(value)
                    break
        
        # Padding
        max_comps = 5
        while len(comp_ids) < max_comps:
            comp_ids.append(0)
        
        comp_ids = comp_ids[:max_comps]
        
        return torch.tensor([comp_ids], dtype=torch.long)
    
    def get_criticality_level(self, score: float) -> str:
        """
        Convierte score numérico a nivel de criticidad
        
        Returns:
            "CRITICAL", "HIGH", "MEDIUM", "LOW"
        """
        if score >= self.threshold_critical:
            return "CRITICAL"
        elif score >= self.threshold_high:
            return "HIGH"
        elif score >= self.threshold_medium:
            return "MEDIUM"
        else:
            return "LOW"
    
    def batch_classify(self, flows: List[Any]) -> List[Tuple[Any, float, str]]:
        """
        Clasifica múltiples flujos en batch para eficiencia
        
        Returns:
            Lista de (flow, score, criticality_level)
        """
        results = []
        
        # TODO: Implementar batching real para mejor rendimiento
        for flow in flows:
            score = self.classify_flow(flow)
            level = self.get_criticality_level(score)
            results.append((flow, score, level))
        
        return results


class MultiObjectiveLoss(nn.Module):
    """
    Función de pérdida multi-objetivos para entrenamiento
    Combina: BCE + Ranking Loss + Regularización + Context Loss
    """
    
    def __init__(self, weights: Dict[str, float]):
        super().__init__()
        self.weights = weights
        self.bce_loss = nn.BCELoss()
    
    def forward(
        self,
        predictions: torch.Tensor,
        labels: torch.Tensor,
        scores: torch.Tensor
    ) -> torch.Tensor:
        """
        Args:
            predictions: Predicciones del modelo [batch]
            labels: Labels binarios [batch]
            scores: Scores de criticidad para ranking [batch]
        """
        # 1. Binary Cross Entropy
        bce = self.bce_loss(predictions, labels)
        
        # 2. Ranking Loss (pairwise)
        ranking = self._ranking_loss(predictions, scores)
        
        # 3. Regularización (L2 implícito en optimizer)
        regularization = torch.tensor(0.0, device=predictions.device)
        
        # 4. Context Loss (consistencia de scores similares)
        context = self._context_loss(predictions, scores)
        
        # Combinar con pesos
        total_loss = (
            self.weights['bce'] * bce +
            self.weights['ranking'] * ranking +
            self.weights['regularization'] * regularization +
            self.weights['context'] * context
        )
        
        return total_loss
    
    def _ranking_loss(self, predictions: torch.Tensor, scores: torch.Tensor) -> torch.Tensor:
        """Penaliza ordenamiento incorrecto de criticidad"""
        # Implementación simplificada
        return torch.tensor(0.0, device=predictions.device)
    
    def _context_loss(self, predictions: torch.Tensor, scores: torch.Tensor) -> torch.Tensor:
        """Fuerza consistencia contextual"""
        # Implementación simplificada
        return torch.tensor(0.0, device=predictions.device)
